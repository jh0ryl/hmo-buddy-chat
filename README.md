# HMO RAG System ğŸ¥

A Retrieval-Augmented Generation (RAG) system for HMO documentation that uses Ollama for local LLM inference and provides intelligent responses based on your organization's documents.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸŒŸ Features

- **ğŸ“š Document-Based Responses**: Answers queries using your HMO documentation
- **ğŸ” Smart Retrieval**: Intelligent context retrieval with similarity scoring
- **ğŸ’¬ Conversational AI**: Maintains context across multiple turns
- **ğŸš€ Fast & Local**: Runs locally using Ollama (no API costs)
- **ğŸ”§ Easy Setup**: Simple installation with minimal dependencies
- **ğŸ¯ Accurate**: Enhanced prompt engineering for better context utilization
- **ğŸ“Š Debug Tools**: Built-in diagnostics and testing utilities

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI   â”‚ â† REST API Server
â”‚   Backend   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚  RAG Service â”‚   â”‚  Vector   â”‚
â”‚   (Ollama)   â”‚   â”‚   Store   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚                 â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Documents    â”‚
         â”‚  (HMO Files)   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Prerequisites

- Python 3.8 or higher
- [Ollama](https://ollama.ai/) installed and running
- 4GB+ RAM recommended
- Windows, macOS, or Linux

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/hmo-rag-system.git
cd hmo-rag-system
```

### 2. Install Dependencies

**Option A: Simple Installation (No C++ build tools required)**
```bash
pip install -r requirements_simple.txt
```

**Option B: Full Installation (with ChromaDB)**
```bash
pip install -r requirements.txt
```

### 3. Install Ollama Models

```bash
ollama pull llama3.2
ollama pull mxbai-embed-large
```

### 4. Add Your Documents

Place your HMO documents (`.txt` or `.md` files) in the `backend/documents/` folder:

```
backend/documents/
â”œâ”€â”€ policies.txt
â”œâ”€â”€ faqs.txt
â”œâ”€â”€ contracts.txt
â”œâ”€â”€ sops.txt
â”œâ”€â”€ appointment.txt
â””â”€â”€ sample_hmo_info.txt
```

### 5. Setup the System

```bash
cd backend
python setup_rag.py
```

This will:
- Check your project structure
- Load and chunk your documents
- Create embeddings
- Test the system

### 6. Start the Server

```bash
python main.py
```

The API will be available at `http://localhost:8000`

### 7. Test It!

```bash
# Check status
curl http://localhost:8000/status

# Query the system
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"query": "What are the HMO benefits?"}'
```

## ğŸ“ Project Structure

```
hmo-rag-system/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                      # FastAPI server
â”‚   â”œâ”€â”€ improved_rag_service.py      # Enhanced RAG service
â”‚   â”œâ”€â”€ vector_store.py              # Vector storage (Simple or ChromaDB)
â”‚   â”œâ”€â”€ load_documents.py            # Document loader with chunking
â”‚   â”œâ”€â”€ setup_rag.py                 # Setup script
â”‚   â”œâ”€â”€ quick_test.py                # Quick testing
â”‚   â”œâ”€â”€ diagnose_rag.py             # Diagnostic tools
â”‚   â”œâ”€â”€ example_usage.py            # Usage examples
â”‚   â”œâ”€â”€ documents/                   # Your HMO documents
â”‚   â”‚   â”œâ”€â”€ policies.txt
â”‚   â”‚   â”œâ”€â”€ faqs.txt
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ simple_vector_data/          # Vector database
â”œâ”€â”€ src/                             # Frontend (Next.js/React)
â”‚   â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ pages/
â”‚   â””â”€â”€ services/
â”œâ”€â”€ requirements.txt                 # Full dependencies
â”œâ”€â”€ requirements_simple.txt          # Minimal dependencies
â”œâ”€â”€ README.md                        # This file
```

## ğŸ”Œ API Endpoints

### GET `/status`
Get system status and document count.

**Response:**
```json
{
  "status": "operational",
  "document_count": 42,
  "collection_name": "hmo_documents"
}
```

### POST `/query`
Query the RAG system.

**Request:**
```json
{
  "query": "What are the coverage benefits?",
  "use_context": true,
  "n_context_docs": 6,
  "min_similarity": 0.0,
  "temperature": 0.7
}
```

**Response:**
```json
{
  "response": "Based on the HMO documentation...",
  "contexts_used": 5
}
```

### POST `/chat`
Chat with conversation history.

**Request:**
```json
{
  "query": "How much does it cost?",
  "conversation_history": [
    {"role": "user", "content": "What plans are available?"},
    {"role": "assistant", "content": "We offer..."}
  ],
  "use_context": true
}
```

### GET `/contexts/{query}`
Debug endpoint to see retrieved contexts.

### GET `/debug/{query}`
Detailed debugging information.

### POST `/add-documents`
Add documents programmatically.

### DELETE `/reset`
Reset the vector store.

## ğŸ¨ Frontend Integration

### React/Next.js Example

```typescript
async function queryRAG(question: string) {
  const response = await fetch('http://localhost:8000/query', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      query: question,
      use_context: true,
      n_context_docs: 6
    })
  });
  
  const data = await response.json();
  return data.response;
}
```

### With Conversation History

```typescript
const [history, setHistory] = useState([]);

async function chat(message: string) {
  const response = await fetch('http://localhost:8000/chat', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      query: message,
      conversation_history: history,
      use_context: true
    })
  });
  
  const data = await response.json();
  
  // Update history
  setHistory([
    ...history,
    { role: 'user', content: message },
    { role: 'assistant', content: data.response }
  ]);
  
  return data.response;
}
```

## âš™ï¸ Configuration

### Adjust Chunking Strategy

Edit `load_documents.py`:

```python
loader = DocumentLoader(
    chunk_size=1000,      # Characters per chunk
    chunk_overlap=200     # Overlap between chunks
)
```

### Adjust Retrieval Settings

```python
response = rag.generate_response(
    query,
    n_context_docs=10,    # Number of contexts to retrieve
    min_similarity=0.3,   # Minimum similarity threshold
    temperature=0.7       # Model temperature (0.0-1.0)
)
```

### Use Different Models

```python
rag = ImprovedRAGService(
    llm_model="llama3.1:8b",           # Different LLM
    embedding_model="mxbai-embed-large" # Different embeddings
)
```

## ğŸ§ª Testing

### Quick Test
```bash
python quick_test.py
```

### Full Diagnostics
```bash
python diagnose_rag.py
```

### Test Specific Query
```python
from improved_rag_service import ImprovedRAGService

rag = ImprovedRAGService()
rag.interactive_debug("Your question here")
```

## ğŸ› Troubleshooting

### No documents loaded
```bash
# Check documents folder
ls backend/documents/

# Reload documents
cd backend
python load_documents.py
```

### Poor responses
- Increase `n_context_docs` (try 8-10)
- Lower `min_similarity` (try 0.0-0.3)
- Adjust `temperature` (lower = more focused)
- Use a larger model (llama3.1:8b or llama2:13b)

### ChromaDB installation issues (Windows)
Use the simple vector store instead:
```bash
pip install -r requirements_simple.txt
# Replace vector_store.py with simple_vector_store.py
```

## ğŸ“Š Performance Tips

- **Chunk Size**: 1000-1500 characters for most documents
- **Chunk Overlap**: 200-300 characters for context continuity
- **Context Docs**: 6-10 for comprehensive answers
- **Temperature**: 0.3-0.5 for factual responses, 0.7-0.9 for creative
- **Model**: Use llama3.1:8b or larger for better accuracy

## ğŸ”’ Security

- Runs completely locally (no data sent to external APIs)
- No API keys required
- Documents stay on your machine

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“§ Support

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai/) - Local LLM inference
- [FastAPI](https://fastapi.tiangolo.com/) - Modern web framework
- [ChromaDB](https://www.trychroma.com/) / Simple Vector Store - Vector database
- The open-source AI community


## â­ Star History

If you find this project helpful, please consider giving it a star!

---

Made with â¤ï¸ for better HMO documentation access
